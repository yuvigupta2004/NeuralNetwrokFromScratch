{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "mnist_data = fetch_openml(\"mnist_784\")\n",
    "x = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "x /= 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "num_labels = 10\n",
    "examples = y.shape[0]\n",
    "\n",
    "# Convert 'y' to numpy array\n",
    "y_array = np.array(y.astype('int32'))\n",
    "\n",
    "# Define the one_hot function\n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)\n",
    "\n",
    "# Apply one-hot encoding\n",
    "y_new = one_hot(y_array, num_labels)\n",
    "\n",
    "# Split, reshape, and shuffle\n",
    "train_size = 60000\n",
    "test_size = x.shape[0] - train_size\n",
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y_new[:train_size], y_new[train_size:]\n",
    "\n",
    "# Convert x_train and y_train to NumPy arrays (if they are not already)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_index = np.random.permutation(train_size)\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "# Now x_train and y_train are ready to be used for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 784) (60000, 10)\n",
      "Test data: (10000, 784) (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, activation='sigmoid'):\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is currently not support, please use 'relu' or 'sigmoid' instead.\")\n",
    "        \n",
    "        # Save all weights\n",
    "        self.params = self.initialize()\n",
    "        # Save all intermediate values, i.e. activations\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        '''\n",
    "            Derivative of ReLU is a bit more complicated since it is not differentiable at x = 0\n",
    "        \n",
    "            Forward path:\n",
    "            relu(x) = max(0, x)\n",
    "            In other word,\n",
    "            relu(x) = 0, if x < 0\n",
    "                    = x, if x >= 0\n",
    "\n",
    "            Backward path:\n",
    "            ∇relu(x) = 0, if x < 0\n",
    "                     = 1, if x >=0\n",
    "        '''\n",
    "        if derivative:\n",
    "            x = np.where(x < 0, 0, x)\n",
    "            x = np.where(x >= 0, 1, x)\n",
    "            return x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            σ(x) = 1 / 1+exp(-z)\n",
    "            \n",
    "            Backward path:\n",
    "            ∇σ(x) = exp(-z) / (1+exp(-z))^2\n",
    "        '''\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "            softmax(x) = exp(x) / ∑exp(x)\n",
    "        '''\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialize(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_layer=self.sizes[1]\n",
    "        output_layer=self.sizes[2]\n",
    "        \n",
    "        params = {\n",
    "            \"W1\": np.random.randn(hidden_layer, input_layer) * np.sqrt(1./input_layer),\n",
    "            \"b1\": np.zeros((hidden_layer, 1)) * np.sqrt(1./input_layer),\n",
    "            \"W2\": np.random.randn(output_layer, hidden_layer) * np.sqrt(1./hidden_layer),\n",
    "            \"b2\": np.zeros((output_layer, 1)) * np.sqrt(1./hidden_layer)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def initialize_momemtum_optimizer(self):\n",
    "        momemtum_opt = {\n",
    "            \"W1\": np.zeros(self.params[\"W1\"].shape),\n",
    "            \"b1\": np.zeros(self.params[\"b1\"].shape),\n",
    "            \"W2\": np.zeros(self.params[\"W2\"].shape),\n",
    "            \"b2\": np.zeros(self.params[\"b2\"].shape),\n",
    "        }\n",
    "        return momemtum_opt\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        '''\n",
    "            y = σ(wX + b)\n",
    "        '''\n",
    "        self.cache[\"X\"] = x\n",
    "        self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) + self.params[\"b1\"]\n",
    "        self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "        self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) + self.params[\"b2\"]\n",
    "        self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "        return self.cache[\"A2\"]\n",
    "    \n",
    "    def back_propagate(self, y, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        current_batch_size = y.shape[0]\n",
    "        \n",
    "        dZ2 = output - y.T\n",
    "        dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "        db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "        dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "        dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "        db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "        return self.grads\n",
    "    \n",
    "    def cross_entropy_loss(self, y, output):\n",
    "        '''\n",
    "            L(y, ŷ) = −∑ylog(ŷ).\n",
    "        '''\n",
    "        l_sum = np.sum(np.multiply(y.T, np.log(output)))\n",
    "        m = y.shape[0]\n",
    "        l = -(1./m) * l_sum\n",
    "        return l\n",
    "                \n",
    "    def optimize(self, l_rate=0.1, beta=.9):\n",
    "        '''\n",
    "            Stochatic Gradient Descent (SGD):\n",
    "            θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "            \n",
    "            Momentum:\n",
    "            v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "            θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "        '''\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for key in self.params:\n",
    "                self.params[key] = self.params[key] - l_rate * self.grads[key]\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for key in self.params:\n",
    "                self.momemtum_opt[key] = (beta * self.momemtum_opt[key] + (1. - beta) * self.grads[key])\n",
    "                self.params[key] = self.params[key] - l_rate * self.momemtum_opt[key]\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer is currently not support, please use 'sgd' or 'momentum' instead.\")\n",
    "\n",
    "    def accuracy(self, y, output):\n",
    "        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs=10, \n",
    "              batch_size=64, optimizer='momentum', l_rate=0.1, beta=.9):\n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        num_batches = -(-x_train.shape[0] // self.batch_size)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optimizer\n",
    "        if self.optimizer == 'momentum':\n",
    "            self.momemtum_opt = self.initialize_momemtum_optimizer()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        template = \"Epoch {}: {:.2f}s, train acc={:.2f}, train loss={:.2f}, test acc={:.2f}, test loss={:.2f}\"\n",
    "        \n",
    "        # Train\n",
    "        for i in range(self.epochs):\n",
    "            # Shuffle\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            for j in range(num_batches):\n",
    "                # Batch\n",
    "                begin = j * self.batch_size\n",
    "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "                x = x_train_shuffled[begin:end]\n",
    "                y = y_train_shuffled[begin:end]\n",
    "                \n",
    "                # Forward\n",
    "                output = self.feed_forward(x)\n",
    "                # Backprop\n",
    "                grad = self.back_propagate(y, output)\n",
    "                # Optimize\n",
    "                self.optimize(l_rate=l_rate, beta=beta)\n",
    "\n",
    "            # Evaluate performance\n",
    "            # Training data\n",
    "            output = self.feed_forward(x_train)\n",
    "            train_acc = self.accuracy(y_train, output)\n",
    "            train_loss = self.cross_entropy_loss(y_train, output)\n",
    "            # Test data\n",
    "            output = self.feed_forward(x_test)\n",
    "            test_acc = self.accuracy(y_test, output)\n",
    "            test_loss = self.cross_entropy_loss(y_test, output)\n",
    "            \n",
    "                        # Assuming test_loss is a Series or NumPy array with multiple values\n",
    "            if isinstance(test_loss, pd.Series):\n",
    "                test_loss = test_loss.mean()  # Use mean, or choose another method\n",
    "            elif isinstance(test_loss, np.ndarray):\n",
    "                test_loss = np.mean(test_loss)  # Use mean, or choose another method\n",
    "\n",
    "            # print(\"Test Loss:\", test_loss)\n",
    "            # print(\"Shape of Test Loss:\", test_loss.shape)\n",
    "\n",
    "           \n",
    "            # Ensure that accuracy and loss are scalars\n",
    "            # Ensure scalar conversion if needed\n",
    "            # Assuming these variables might be Series or arrays\n",
    "            train_acc = train_acc.item() if isinstance(train_acc, pd.Series) else train_acc\n",
    "            train_loss = train_loss.item() if isinstance(train_loss, pd.Series) else train_loss\n",
    "            test_acc = test_acc.item() if isinstance(test_acc, pd.Series) else test_acc\n",
    "            test_loss = test_loss.mean() if isinstance(test_loss, (pd.Series, np.ndarray)) else test_loss  # Using mean\n",
    "\n",
    "            # Print the results\n",
    "            template = \"Epoch: {} | Time: {:.2f}s | Train Acc: {:.4f} | Train Loss: {:.4f} | Test Acc: {:.4f} | Test Loss: {:.4f}\"\n",
    "            print(template.format(i+1, time.time()-start_time, train_acc, train_loss, test_acc, test_loss))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 3.00s | Train Acc: 0.9554 | Train Loss: 0.1499 | Test Acc: 0.9523 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Time: 5.76s | Train Acc: 0.9689 | Train Loss: 0.1018 | Test Acc: 0.9643 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Time: 7.90s | Train Acc: 0.9719 | Train Loss: 0.0903 | Test Acc: 0.9635 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Time: 11.57s | Train Acc: 0.9772 | Train Loss: 0.0733 | Test Acc: 0.9694 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Time: 14.55s | Train Acc: 0.9846 | Train Loss: 0.0512 | Test Acc: 0.9718 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Time: 19.59s | Train Acc: 0.9872 | Train Loss: 0.0437 | Test Acc: 0.9722 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Time: 24.06s | Train Acc: 0.9874 | Train Loss: 0.0407 | Test Acc: 0.9722 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Time: 26.56s | Train Acc: 0.9908 | Train Loss: 0.0335 | Test Acc: 0.9752 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Time: 30.38s | Train Acc: 0.9912 | Train Loss: 0.0311 | Test Acc: 0.9750 | Test Loss: 0.0000\n",
      "Epoch: 10 | Time: 32.87s | Train Acc: 0.9934 | Train Loss: 0.0245 | Test Acc: 0.9761 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid + Momentum\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='sigmoid')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='momentum', l_rate=4, beta=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 2.26s | Train Acc: 0.8907 | Train Loss: 0.3955 | Test Acc: 0.8941 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Time: 4.19s | Train Acc: 0.9052 | Train Loss: 0.3293 | Test Acc: 0.9098 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Time: 5.51s | Train Acc: 0.9118 | Train Loss: 0.3044 | Test Acc: 0.9153 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Time: 7.42s | Train Acc: 0.9161 | Train Loss: 0.2886 | Test Acc: 0.9202 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Time: 9.10s | Train Acc: 0.9194 | Train Loss: 0.2796 | Test Acc: 0.9203 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Time: 11.36s | Train Acc: 0.9227 | Train Loss: 0.2688 | Test Acc: 0.9251 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Time: 13.40s | Train Acc: 0.9243 | Train Loss: 0.2630 | Test Acc: 0.9227 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Time: 15.65s | Train Acc: 0.9258 | Train Loss: 0.2567 | Test Acc: 0.9264 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Time: 17.45s | Train Acc: 0.9282 | Train Loss: 0.2516 | Test Acc: 0.9256 | Test Loss: 0.0000\n",
      "Epoch: 10 | Time: 19.60s | Train Acc: 0.9283 | Train Loss: 0.2503 | Test Acc: 0.9254 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "# ReLU + SGD\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='relu')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='sgd', l_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 2.35s | Train Acc: 0.8019 | Train Loss: 0.9558 | Test Acc: 0.8119 | Test Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Time: 5.79s | Train Acc: 0.8606 | Train Loss: 0.6062 | Test Acc: 0.8696 | Test Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Time: 8.38s | Train Acc: 0.8810 | Train Loss: 0.4819 | Test Acc: 0.8884 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Time: 10.29s | Train Acc: 0.8882 | Train Loss: 0.4205 | Test Acc: 0.8954 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Time: 11.85s | Train Acc: 0.8971 | Train Loss: 0.3819 | Test Acc: 0.9024 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Time: 14.80s | Train Acc: 0.9018 | Train Loss: 0.3570 | Test Acc: 0.9054 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Time: 16.53s | Train Acc: 0.9061 | Train Loss: 0.3384 | Test Acc: 0.9100 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Time: 18.77s | Train Acc: 0.9087 | Train Loss: 0.3241 | Test Acc: 0.9145 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Time: 20.99s | Train Acc: 0.9113 | Train Loss: 0.3128 | Test Acc: 0.9160 | Test Loss: 0.0000\n",
      "Epoch: 10 | Time: 23.12s | Train Acc: 0.9141 | Train Loss: 0.3028 | Test Acc: 0.9176 | Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvigupta/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='sigmoid')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='sgd', l_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good exercises in NumPy\n",
    "You might have noticed that the code is very readable, but takes up a lot of space and could be optimized to run in loops. Here is a chance to optimize and improve the code. For example, you can optimize the forward and backward pass, such that they run in a for loop in each function. This makes the code easier to modify and possibly easier to maintain. \n",
    "\n",
    "More challenging exercises including implement any other activation function from this overview of activation functions, and remember to implement the derivatives as well. Different optimizers, e.g. Adam, RMSProp, etc, are also worth to try.\n",
    "\n",
    "## Reference \n",
    "1. [CS565600 Deep Learning](https://nthu-datalab.github.io/ml/index.html), National Tsing Hua University\n",
    "2. [Building a Neural Network from Scratch: Part 1](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/)\n",
    "3. [Building a Neural Network from Scratch: Part 2](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/)\n",
    "4. [Neural networks from scratch](https://developer.ibm.com/technologies/artificial-intelligence/articles/neural-networks-from-scratch), IBM Developer\n",
    "5. [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
